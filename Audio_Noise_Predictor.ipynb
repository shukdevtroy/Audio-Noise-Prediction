{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBeADbCYHWOt",
        "outputId": "f2b3b56a-8763-43c1-cdc8-cb22c5e43526"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code defines a function called `load_data` that loads audio data from a specified directory and converts it into a format suitable for machine learning. It processes audio files organized into subfolders based on their labels, such as 'clean_audio', 'gaussian_noise', and 'impulse_noise'. For each audio file, it uses the `librosa` library to read the audio and extract Mel-frequency cepstral coefficients (MFCCs), which are useful features for audio analysis. The function ensures that all MFCC arrays have a consistent length (defined by `max_length`) by either truncating longer arrays or padding shorter ones with zeros. Finally, it returns two arrays: one containing the processed MFCC data (`X`) and the other containing the corresponding labels (`y`) for each audio file."
      ],
      "metadata": {
        "id": "d7ct3h2IBI1Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import librosa\n",
        "import tensorflow as tf\n",
        "\n",
        "def load_data(data_dir, max_length=100):  # Set a max length for MFCC\n",
        "    X, y = [], []\n",
        "    labels = {'clean_audio': 0, 'gaussian_noise': 1, 'impulse_noise': 2}\n",
        "\n",
        "    for label, index in labels.items():\n",
        "        folder_path = os.path.join(data_dir, label)\n",
        "        for filename in os.listdir(folder_path):\n",
        "            if filename.endswith('.wav'):\n",
        "                file_path = os.path.join(folder_path, filename)\n",
        "                audio, sr = librosa.load(file_path, sr=None)\n",
        "                mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)\n",
        "\n",
        "                # Pad or truncate the MFCCs\n",
        "                if mfccs.shape[1] > max_length:\n",
        "                    mfccs = mfccs[:, :max_length]  # Truncate\n",
        "                elif mfccs.shape[1] < max_length:\n",
        "                    pad_width = max_length - mfccs.shape[1]\n",
        "                    mfccs = np.pad(mfccs, ((0, 0), (0, pad_width)), mode='constant')  # Pad\n",
        "\n",
        "                X.append(mfccs)\n",
        "                y.append(index)\n",
        "\n",
        "    return np.array(X), np.array(y)"
      ],
      "metadata": {
        "id": "ZlkzwmwQHac2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code defines a neural network model using TensorFlow's Keras library and prepares it for training on audio data. First, it creates a function called `build_model` that sets up a sequential model with several layers: a 1D convolutional layer to extract features, a max pooling layer to reduce dimensionality, an LSTM layer to capture temporal patterns, and a dense layer to make predictions. The final layer uses the softmax activation function to classify the data into three categories. The code then loads the audio data using the `load_data` function and reshapes it to fit the LSTM's input requirements. Next, it splits the data into training and validation sets using `train_test_split` to ensure the model can be evaluated properly. Finally, the model is compiled with a loss function and an optimizer, and it is trained on the training set for 30 epochs, while also validating its performance on the validation set."
      ],
      "metadata": {
        "id": "nyhMbQQrBRaH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Flatten, Conv1D, MaxPooling1D, Dropout\n",
        "\n",
        "def build_model(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(Conv1D(32, kernel_size=3, activation='relu', input_shape=input_shape))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(LSTM(64, return_sequences=True))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dropout(0.5))  # Regularization\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(Dense(3, activation='softmax'))  # 3 classes\n",
        "    return model\n",
        "\n",
        "X, y = load_data('/content/drive/MyDrive/Data Directory/')\n",
        "\n",
        "# Reshape X to have the right input shape for LSTM\n",
        "X = np.array([x.reshape(-1, 13) for x in X])  # Reshape for LSTM input\n",
        "\n",
        "# Split your data into training and validation sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = build_model(X_train.shape[1:])\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=30)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGhmwix1Hl1S",
        "outputId": "ac4a2845-e18a-480c-ebda-ee12165d26b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 57ms/step - accuracy: 0.4066 - loss: 1.1030 - val_accuracy: 0.5724 - val_loss: 0.8561\n",
            "Epoch 2/30\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 53ms/step - accuracy: 0.6263 - loss: 0.8023 - val_accuracy: 0.6103 - val_loss: 0.7793\n",
            "Epoch 3/30\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 67ms/step - accuracy: 0.6769 - loss: 0.7185 - val_accuracy: 0.6414 - val_loss: 0.7552\n",
            "Epoch 4/30\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 57ms/step - accuracy: 0.7497 - loss: 0.5860 - val_accuracy: 0.6552 - val_loss: 0.7548\n",
            "Epoch 5/30\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 60ms/step - accuracy: 0.7377 - loss: 0.5791 - val_accuracy: 0.6690 - val_loss: 0.6886\n",
            "Epoch 6/30\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - accuracy: 0.8283 - loss: 0.4573 - val_accuracy: 0.7103 - val_loss: 0.6621\n",
            "Epoch 7/30\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7964 - loss: 0.4625 - val_accuracy: 0.6862 - val_loss: 0.7047\n",
            "Epoch 8/30\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - accuracy: 0.8237 - loss: 0.4021 - val_accuracy: 0.6793 - val_loss: 0.7374\n",
            "Epoch 9/30\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.8234 - loss: 0.4038 - val_accuracy: 0.7207 - val_loss: 0.6286\n",
            "Epoch 10/30\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - accuracy: 0.8885 - loss: 0.3257 - val_accuracy: 0.7345 - val_loss: 0.6030\n",
            "Epoch 11/30\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 59ms/step - accuracy: 0.8615 - loss: 0.3238 - val_accuracy: 0.7310 - val_loss: 0.6467\n",
            "Epoch 12/30\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - accuracy: 0.9064 - loss: 0.2435 - val_accuracy: 0.7483 - val_loss: 0.6820\n",
            "Epoch 13/30\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - accuracy: 0.8814 - loss: 0.3023 - val_accuracy: 0.7517 - val_loss: 0.6518\n",
            "Epoch 14/30\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - accuracy: 0.9147 - loss: 0.2269 - val_accuracy: 0.7483 - val_loss: 0.6164\n",
            "Epoch 15/30\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - accuracy: 0.9202 - loss: 0.2116 - val_accuracy: 0.7828 - val_loss: 0.5831\n",
            "Epoch 16/30\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - accuracy: 0.9141 - loss: 0.2148 - val_accuracy: 0.7517 - val_loss: 0.6486\n",
            "Epoch 17/30\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - accuracy: 0.9238 - loss: 0.1866 - val_accuracy: 0.7483 - val_loss: 0.6715\n",
            "Epoch 18/30\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - accuracy: 0.9344 - loss: 0.1701 - val_accuracy: 0.7276 - val_loss: 0.6853\n",
            "Epoch 19/30\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.9172 - loss: 0.2113 - val_accuracy: 0.7552 - val_loss: 0.6778\n",
            "Epoch 20/30\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step - accuracy: 0.9314 - loss: 0.1346 - val_accuracy: 0.7759 - val_loss: 0.6648\n",
            "Epoch 21/30\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 60ms/step - accuracy: 0.9504 - loss: 0.1141 - val_accuracy: 0.7552 - val_loss: 0.6971\n",
            "Epoch 22/30\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - accuracy: 0.9350 - loss: 0.1407 - val_accuracy: 0.7517 - val_loss: 0.6770\n",
            "Epoch 23/30\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - accuracy: 0.9519 - loss: 0.1251 - val_accuracy: 0.7793 - val_loss: 0.6905\n",
            "Epoch 24/30\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - accuracy: 0.9427 - loss: 0.1335 - val_accuracy: 0.7241 - val_loss: 0.7648\n",
            "Epoch 25/30\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - accuracy: 0.9320 - loss: 0.1481 - val_accuracy: 0.7483 - val_loss: 0.6817\n",
            "Epoch 26/30\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - accuracy: 0.9413 - loss: 0.1269 - val_accuracy: 0.7207 - val_loss: 0.8273\n",
            "Epoch 27/30\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - accuracy: 0.9502 - loss: 0.1130 - val_accuracy: 0.7690 - val_loss: 0.7009\n",
            "Epoch 28/30\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 61ms/step - accuracy: 0.9501 - loss: 0.1142 - val_accuracy: 0.7655 - val_loss: 0.6959\n",
            "Epoch 29/30\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - accuracy: 0.9470 - loss: 0.0989 - val_accuracy: 0.7552 - val_loss: 0.8857\n",
            "Epoch 30/30\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - accuracy: 0.9333 - loss: 0.1401 - val_accuracy: 0.7621 - val_loss: 0.7477\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7c6b8fb5aa10>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "model.save('/content/drive/MyDrive/Data Directory/audio_classification_model.h5')\n",
        "\n",
        "# Load the model\n",
        "loaded_model = tf.keras.models.load_model('/content/drive/MyDrive/Data Directory/audio_classification_model.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nC_Yy1upiul-",
        "outputId": "a25558e0-2ab1-40cd-d38c-89c05d40347b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `predict_audio` function is designed to take an audio file, process it, and predict its class using a trained model. It starts by loading the audio file and extracting its Mel-frequency cepstral coefficients (MFCCs) using the `librosa` library. To ensure the MFCCs are the correct length for the model, it either truncates them if they are too long or pads them with zeros if they are too short. After that, the MFCCs are reshaped to match the input format required by the model. The function then uses the model to predict the class of the audio, obtaining a prediction score for each possible class. It determines the class index with the highest score using `np.argmax`, which corresponds to the predicted class. Finally, the function returns this class index. In the example usage, the function is called with a specific audio file, and the predicted class is printed to the console."
      ],
      "metadata": {
        "id": "dhU_FOBHBYqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_audio(file_path, model, max_length=100):\n",
        "    audio, sr = librosa.load(file_path, sr=None)\n",
        "    mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)\n",
        "\n",
        "    # Pad or truncate the MFCCs\n",
        "    if mfccs.shape[1] > max_length:\n",
        "        mfccs = mfccs[:, :max_length]  # Truncate\n",
        "    elif mfccs.shape[1] < max_length:\n",
        "        pad_width = max_length - mfccs.shape[1]\n",
        "        mfccs = np.pad(mfccs, ((0, 0), (0, pad_width)), mode='constant')  # Pad\n",
        "\n",
        "    mfccs_reshaped = mfccs.reshape(1, max_length, 13)  # Reshape for model input\n",
        "    prediction = model.predict(mfccs_reshaped)\n",
        "    class_index = np.argmax(prediction)\n",
        "    return class_index\n",
        "\n",
        "# Example usage\n",
        "result = predict_audio('/content/drive/MyDrive/Data Directory/noisy_audio_impulse_clean_audio2.wav', loaded_model)\n",
        "print(f'Predicted class: {result}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cat3LLRi60Y",
        "outputId": "de35cf52-41b0-47a4-8175-bea75e6a7687"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200ms/step\n",
            "Predicted class: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Class Description:\n",
        "\n",
        "`labels` = {`clean_audio`: 0, `gaussian_noise`: 1, `impulse_noise`: 2}"
      ],
      "metadata": {
        "id": "slRyJo6pBfjH"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2PYKwTvfChun"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}